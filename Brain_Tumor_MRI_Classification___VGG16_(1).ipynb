{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Brain Tumor MRI Classification _ VGG16 (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seeverblog/brain_tumor_dataset/blob/master/Brain_Tumor_MRI_Classification___VGG16_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "398a72fe-622e-4aa3-9b07-0a4a8351bb73",
        "_cell_guid": "1ffedc08-a3cc-47a4-88a2-f6de6bbf293b",
        "trusted": true,
        "id": "IXSAPFLYd7a-",
        "colab_type": "text"
      },
      "source": [
        "**<center><font size=5>Brain Tumor Detection with Transefer Learning </font></center>**\n",
        "***\n",
        "**author**: Ruslan Klymentiev, Loaii abdalslam\n",
        "\n",
        "**date**: \n",
        "- 14th June, 2019 Ruslan Klymentiev\n",
        "- 21th June, 2019 Loai abdalsalm\n",
        "\n",
        "**Table of Contents**\n",
        "- <a href='#intro'>1. Project Overview and Objectives</a> \n",
        "    - <a href='#dataset'>1.1. Data Set Description</a>\n",
        "    - <a href='#tumor'>1.2. What is Brain Tumor?</a>\n",
        "- <a href='#env'>2. Setting up the Environment</a>\n",
        "- <a href='#import'>3. Data Import and Preprocessing</a>\n",
        "- <a href='#cnn'>4. CNN Model</a>\n",
        "    - <a href='#aug'>4.1. Data Augmentation</a>\n",
        "        - <a href='#demo'>4.1.1. Demo</a>\n",
        "        - <a href='#apply'>4.1.2. Apply</a>\n",
        "    - <a href='#build'>4.2. Transfer Learning Tutorial </a>\n",
        "    - <a href='#perf'>4.3. Model Performance</a>\n",
        "- <a href='#concl'>5. Conclusions</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1704a28b-d08b-4eed-a2a7-6f3cbec42750",
        "_cell_guid": "5d764a3b-7eb1-4585-b355-4572bb06504f",
        "trusted": true,
        "id": "2dVubfIYd7bN",
        "colab_type": "text"
      },
      "source": [
        "# <a id='intro'>1. Project Overview and Objectives</a>\n",
        "\n",
        "The main purpose of this project was to build a CNN model that would classify if subject has a tumor or not base on MRI scan. I used the VGG-16, Inception v3 , xception model architecture and weights to train the model for this binary problem. I used `accuracy` as a metric to justify the model performance which can be defined as:\n",
        "\n",
        "$\\textrm{Accuracy} = \\frac{\\textrm{Number of correclty predicted images}}{\\textrm{Total number of tested images}} \\times 100\\%$\n",
        "\n",
        "Final results look as follows:\n",
        "\n",
        "| Set | Accuracy |\n",
        "|:-:|:-:|\n",
        "| Validation Set* | ~92% |\n",
        "| Test Set* | ~92% |\n",
        "<br>\n",
        "\\* *Note: there might be some misunderstanding in terms of set names so I want to describe what do I mean by `test` and `validation` set:*\n",
        "* *`validation set` - is the set used during the model training to adjust the hyperparameters. *\n",
        "* *`test set` - is the small set that I don't touch for the whole training process at all. It's been used for final model performance evaluation.*\n",
        "\n",
        "## <a id='dataset'>1.1. Data Set Description</a>\n",
        "\n",
        "The image data that was used for this problem is [Brain MRI Images for Brain Tumor Detection](https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection). It conists of MRI scans of two classes:\n",
        "\n",
        "* `NO` - no tumor, encoded as `0`\n",
        "* `YES` - tumor, encoded as `1`\n",
        "\n",
        "Unfortunately, the data set description doesn't hold any information where this MRI scans come from and so on.\n",
        "\n",
        "## <a id='tumor'>1.2. What is Brain Tumor?</a>\n",
        "\n",
        "> A brain tumor occurs when abnormal cells form within the brain. There are two main types of tumors: cancerous (malignant) tumors and benign tumors. Cancerous tumors can be divided into primary tumors, which start within the brain, and secondary tumors, which have spread from elsewhere, known as brain metastasis tumors. All types of brain tumors may produce symptoms that vary depending on the part of the brain involved. These symptoms may include headaches, seizures, problems with vision, vomiting and mental changes. The headache is classically worse in the morning and goes away with vomiting. Other symptoms may include difficulty walking, speaking or with sensations. As the disease progresses, unconsciousness may occur.\n",
        ">\n",
        "> ![](https://upload.wikimedia.org/wikipedia/commons/5/5f/Hirnmetastase_MRT-T1_KM.jpg)\n",
        ">\n",
        "> *Brain metastasis in the right cerebral hemisphere from lung cancer, shown on magnetic resonance imaging.*\n",
        "\n",
        "Source: [Wikipedia](https://en.wikipedia.org/wiki/Brain_tumor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5844f3a9-25ed-49c8-93d4-9a9d4194138c",
        "_cell_guid": "fa864c84-c873-4a22-8a9a-a29655350ca9",
        "trusted": true,
        "id": "viiWV9_2d7bS",
        "colab_type": "text"
      },
      "source": [
        "# <a id='env'>2. Setting up the Environment</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "53029304-94c8-4a29-8f70-c5acdcea2354",
        "_cell_guid": "a91306ae-dd23-40f7-a18a-e96cc0867dd1",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "P79s65vud7bb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "b24d82c9-dc53-45ca-a075-b64b8dff7451"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install imutils\n",
        "clear_output()\n",
        "\n",
        "from keras.applications.vgg19 import VGG19,preprocess_input\n",
        "from keras.applications.xception import Xception,preprocess_input\n",
        "from keras.applications.inception_v3 import InceptionV3,inception_v3\n",
        "from keras.applications.resnet50 import ResNet50,resnet50"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "6fdd5028-0ea0-4c33-80de-4f77345193de",
        "_cell_guid": "17a8c783-a858-451e-ae53-44028113a96e",
        "trusted": true,
        "id": "acfWqHOTd7bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "739e229c-a19e-45c8-89f6-96c70a97efba",
        "_cell_guid": "2ad44eb0-2ef2-4074-87e4-be7d7b0a2283",
        "trusted": true,
        "id": "82IzX-cKd7b9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8d3df953-f686-4c27-e2e2-6fc1240f58e5"
      },
      "source": [
        "import numpy as np \n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import itertools\n",
        "import imutils\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras import layers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "RANDOM_SEED = 123"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8639e39b-d9f7-4c05-8d45-8bf6129244f3",
        "_cell_guid": "988d4a73-ae5c-44b6-be4b-1975db75256d",
        "trusted": true,
        "id": "t9YWlkUjd7cQ",
        "colab_type": "text"
      },
      "source": [
        "Right now all images are in one folder with `yes` and `no` subfolders. I will split the data into `train`, `val` and `test` folders which makes its easier to work for me. The new folder heirarchy will look as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "11cd7104-b494-462c-9362-35cb991283a0",
        "_cell_guid": "d444629f-b38f-4e45-aef0-0d8b2625e02b",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "bBRR8Kttd7cT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "55b01aa7-9080-4bd8-db52-88bd6b5b8849"
      },
      "source": [
        "!apt-get install tree\n",
        "#clear_output()\n",
        "# create new folders\n",
        "!mkdir TRAIN TEST VAL TRAIN/YES TRAIN/NO TEST/YES TEST/NO VAL/YES VAL/NO\n",
        "!tree -d"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 7%\r\rReading package lists... 64%\r\rReading package lists... 64%\r\rReading package lists... 65%\r\rReading package lists... 65%\r\rReading package lists... 68%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 72%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 82%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (120 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 134448 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            ".\n",
            "├── sample_data\n",
            "├── TEST\n",
            "│   ├── NO\n",
            "│   └── YES\n",
            "├── TRAIN\n",
            "│   ├── NO\n",
            "│   └── YES\n",
            "└── VAL\n",
            "    ├── NO\n",
            "    └── YES\n",
            "\n",
            "10 directories\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7b499e1f-a113-4bfe-87a4-d5145656ab30",
        "_cell_guid": "21b12f94-1b93-4a8f-8a34-f19222a3eca0",
        "trusted": true,
        "id": "MN954r5td7ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_PATH = '../input/brain-mri-images-for-brain-tumor-detection/brain_tumor_dataset/'\n",
        "# split the data by train/val/test\n",
        "for CLASS in os.listdir(IMG_PATH):\n",
        "    if not CLASS.startswith('.'):\n",
        "        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n",
        "        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n",
        "            img = IMG_PATH + CLASS + '/' + FILE_NAME\n",
        "            if n < 5:\n",
        "                shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n",
        "            elif n < 0.8*IMG_NUM:\n",
        "                shutil.copy(img, 'TRAIN/'+ CLASS.upper() + '/' + FILE_NAME)\n",
        "            else:\n",
        "                shutil.copy(img, 'VAL/'+ CLASS.upper() + '/' + FILE_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "34d6134e-07e9-4565-92ff-23124f9bc794",
        "_cell_guid": "d8c1dc69-9f60-4cef-8d26-92cd1d257119",
        "trusted": true,
        "id": "-SMZgHDKd7cw",
        "colab_type": "text"
      },
      "source": [
        "# <a id='import'>3. Data Import and Preprocessing</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "753e0f77-02dd-4a32-9725-e177d8aaf468",
        "_cell_guid": "8f95f43e-280a-47c5-a4fc-fec5213283ff",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "uU9V-Fpsd7cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(dir_path, img_size=(100,100)):\n",
        "    \"\"\"\n",
        "    Load resized images as np.arrays to workspace\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    i = 0\n",
        "    labels = dict()\n",
        "    for path in tqdm(sorted(os.listdir(dir_path))):\n",
        "        if not path.startswith('.'):\n",
        "            labels[i] = path\n",
        "            for file in os.listdir(dir_path + path):\n",
        "                if not file.startswith('.'):\n",
        "                    img = cv2.imread(dir_path + path + '/' + file)\n",
        "                    X.append(img)\n",
        "                    y.append(i)\n",
        "            i += 1\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    print(f'{len(X)} images loaded from {dir_path} directory.')\n",
        "    return X, y, labels\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize = (6,6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    cm = np.round(cm,2)\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3cdd5d11-e7fd-41a8-901f-2385d009a09c",
        "_cell_guid": "99b2a2b8-72e3-4a56-bd5e-fb689f11de4d",
        "trusted": true,
        "id": "Zhnphh-nd7c7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DIR = 'TRAIN/'\n",
        "TEST_DIR = 'TEST/'\n",
        "VAL_DIR = 'VAL/'\n",
        "IMG_SIZE = (224,224)\n",
        "\n",
        "# use predefined function to load the image data into workspace\n",
        "X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\n",
        "X_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\n",
        "X_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f9d9c36d-92f2-4d0d-a28f-02a495e31900",
        "_cell_guid": "aba738f2-8e5a-4601-a520-b3f4e7a6bea0",
        "trusted": true,
        "id": "c04K4rPFd7dB",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at the distribution of classes among sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8dc5dad8-4d12-4a5c-b5e1-28b2048a590d",
        "_cell_guid": "482123e9-9008-4d54-aa85-d9ee3dea4639",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "luXvj2J9d7dE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = dict()\n",
        "y[0] = []\n",
        "y[1] = []\n",
        "for set_name in (y_train, y_val, y_test):\n",
        "    y[0].append(np.sum(set_name == 0))\n",
        "    y[1].append(np.sum(set_name == 1))\n",
        "\n",
        "trace0 = go.Bar(\n",
        "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
        "    y=y[0],\n",
        "    name='No',\n",
        "    marker=dict(color='#33cc33'),\n",
        "    opacity=0.7\n",
        ")\n",
        "trace1 = go.Bar(\n",
        "    x=['Train Set', 'Validation Set', 'Test Set'],\n",
        "    y=y[1],\n",
        "    name='Yes',\n",
        "    marker=dict(color='#ff3300'),\n",
        "    opacity=0.7\n",
        ")\n",
        "data = [trace0, trace1]\n",
        "layout = go.Layout(\n",
        "    title='Count of classes in each set',\n",
        "    xaxis={'title': 'Set'},\n",
        "    yaxis={'title': 'Count'}\n",
        ")\n",
        "fig = go.Figure(data, layout)\n",
        "iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e9fe529e-932a-4849-ad90-3e3ec5de10b8",
        "_cell_guid": "db1346dd-390b-4271-a57d-e6e720143bf6",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "4UnFJqoqd7dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_samples(X, y, labels_dict, n=50):\n",
        "    \"\"\"\n",
        "    Creates a gridplot for desired number of images (n) from the specified set\n",
        "    \"\"\"\n",
        "    for index in range(len(labels_dict)):\n",
        "        imgs = X[np.argwhere(y == index)][:n]\n",
        "        j = 10\n",
        "        i = int(n/j)\n",
        "\n",
        "        plt.figure(figsize=(15,6))\n",
        "        c = 1\n",
        "        for img in imgs:\n",
        "            plt.subplot(i,j,c)\n",
        "            plt.imshow(img[0])\n",
        "\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            c += 1\n",
        "        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7600bae7-a52f-4d6a-bc58-799bce8b2b9a",
        "_cell_guid": "1d3a3034-2ecb-4eaa-bff1-b1e26ee36196",
        "trusted": true,
        "id": "uzRw9ELXd7dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_samples(X_train, y_train, labels, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "dfe83ca8-b98c-4904-82a7-d7b43f0e2e23",
        "_cell_guid": "3dff1695-76d1-4369-bab1-bcb397efed53",
        "trusted": true,
        "id": "ikpwuxbod7dg",
        "colab_type": "text"
      },
      "source": [
        "As you can see, images have different `width` and `height` and diffent size of \"black corners\". Since the image size for VGG-16 imput layer is `(224,224)` some wide images may look weird after resizing. Histogram of ratio distributions (`ratio = width/height`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b7a1da72-9a48-4186-aef2-1f2132fd3f1e",
        "_cell_guid": "88841ccc-9e9a-43f3-83cc-c4a86b7e98bc",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "l_J_1C5Jd7dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RATIO_LIST = []\n",
        "for set in (X_train, X_test, X_val):\n",
        "    for img in set:\n",
        "        RATIO_LIST.append(img.shape[1]/img.shape[0])\n",
        "        \n",
        "plt.hist(RATIO_LIST)\n",
        "plt.title('Distribution of Image Ratios')\n",
        "plt.xlabel('Ratio Value')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b8075277-b680-4fb5-a48e-004113b28afd",
        "_cell_guid": "6127c258-1bd2-405e-a3ec-60d21a863ee4",
        "trusted": true,
        "id": "CcIpwsw_d7ds",
        "colab_type": "text"
      },
      "source": [
        "The first step of \"normalization\" would be to crop the brain out of the images. I used technique which was perfectly described in [pyimagesearch](https://www.pyimagesearch.com/2016/04/11/finding-extreme-points-in-contours-with-opencv/) blog and I highly suggest to looks deeper into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5579281f-9848-474b-a0cc-2e88b7265520",
        "_cell_guid": "50d752d2-cad0-486c-a3b0-775049406ded",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "Eyd7CpYkd7dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_imgs(set_name, add_pixels_value=0):\n",
        "    \"\"\"\n",
        "    Finds the extreme points on the image and crops the rectangular out of them\n",
        "    \"\"\"\n",
        "    set_new = []\n",
        "    for img in set_name:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "        # threshold the image, then perform a series of erosions +\n",
        "        # dilations to remove any small regions of noise\n",
        "        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "        thresh = cv2.erode(thresh, None, iterations=2)\n",
        "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "        # find contours in thresholded image, then grab the largest one\n",
        "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cnts = imutils.grab_contours(cnts)\n",
        "        c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "        # find the extreme points\n",
        "        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "        extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "        extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "        extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\n",
        "        ADD_PIXELS = add_pixels_value\n",
        "        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
        "        set_new.append(new_img)\n",
        "\n",
        "    return np.array(set_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8e97b016-06ce-4047-9762-ab4914c21e49",
        "_cell_guid": "23549590-0175-4ed7-9656-65d605a034fc",
        "trusted": true,
        "id": "BzyRrZrTd7d3",
        "colab_type": "text"
      },
      "source": [
        "Let's look at example what this function will do with MRI scans:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8b8ab74b-f260-40b0-80a1-308cc45185a3",
        "_cell_guid": "fc06713d-bd99-4728-8ff8-c0dddfcef957",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "Da2tIONad7d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = cv2.imread('../input/brain-mri-images-for-brain-tumor-detection/brain_tumor_dataset/yes/Y108.jpg')\n",
        "img = cv2.resize(\n",
        "            img,\n",
        "            dsize=IMG_SIZE,\n",
        "            interpolation=cv2.INTER_CUBIC\n",
        "        )\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# threshold the image, then perform a series of erosions +\n",
        "# dilations to remove any small regions of noise\n",
        "thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "thresh = cv2.erode(thresh, None, iterations=2)\n",
        "thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "# find contours in thresholded image, then grab the largest one\n",
        "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cnts = imutils.grab_contours(cnts)\n",
        "c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "# find the extreme points\n",
        "extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\n",
        "# add contour on the image\n",
        "img_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n",
        "\n",
        "# add extreme points\n",
        "img_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\n",
        "img_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\n",
        "img_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\n",
        "img_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n",
        "\n",
        "# crop\n",
        "ADD_PIXELS = 0\n",
        "new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "005b3a54-3704-4a24-a8e4-862d69b5a6b8",
        "_cell_guid": "b7b105ff-dddf-4f79-b14e-2bb56ed4ab2b",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "jKRoOR8Ed7eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(141)\n",
        "plt.imshow(img)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 1. Get the original image')\n",
        "plt.subplot(142)\n",
        "plt.imshow(img_cnt)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 2. Find the biggest contour')\n",
        "plt.subplot(143)\n",
        "plt.imshow(img_pnt)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 3. Find the extreme points')\n",
        "plt.subplot(144)\n",
        "plt.imshow(new_img)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Step 4. Crop the image')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "506dabdc-59a9-4da9-b68f-04e68baa51ed",
        "_cell_guid": "3ff77e26-7b68-497c-aef4-2e2cded7d28b",
        "trusted": true,
        "id": "jzs7Ye1Jd7eR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply this for each set\n",
        "X_train_crop = crop_imgs(set_name=X_train)\n",
        "X_val_crop = crop_imgs(set_name=X_val)\n",
        "X_test_crop = crop_imgs(set_name=X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a567c184-298e-44e9-aa8b-629d7ff34bb4",
        "_cell_guid": "41248101-6080-4c60-ae67-303bde252c00",
        "trusted": true,
        "id": "rUkYX52Wd7eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_samples(X_train_crop, y_train, labels, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "294c966f-b108-47c0-bec3-4693229ffb5c",
        "_cell_guid": "42194da2-ee0e-49cc-a4e7-ba8bde36931f",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "vPqZPRDXd7ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_new_images(x_set, y_set, folder_name):\n",
        "    i = 0\n",
        "    for (img, imclass) in zip(x_set, y_set):\n",
        "        if imclass == 0:\n",
        "            cv2.imwrite(folder_name+'NO/'+str(i)+'.jpg', img)\n",
        "        else:\n",
        "            cv2.imwrite(folder_name+'YES/'+str(i)+'.jpg', img)\n",
        "        i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f02c6866-4546-4e73-a84f-33917215de9b",
        "_cell_guid": "2652ab05-cb0a-4ac7-a50c-f90ecde82085",
        "trusted": true,
        "id": "dT6T6yOCd7eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving new images to the folder\n",
        "!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP/YES TRAIN_CROP/NO TEST_CROP/YES TEST_CROP/NO VAL_CROP/YES VAL_CROP/NO\n",
        "\n",
        "save_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP/')\n",
        "save_new_images(X_val_crop, y_val, folder_name='VAL_CROP/')\n",
        "save_new_images(X_test_crop, y_test, folder_name='TEST_CROP/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c752cfad-7713-44ea-90ee-eab149ddf5b9",
        "_cell_guid": "047a62a5-b9db-4e53-b2c3-5a262fbe7c1f",
        "trusted": true,
        "id": "DBDy-acEd7e3",
        "colab_type": "text"
      },
      "source": [
        "The next step would be resizing images to `(224,224)` and applying preprocessing needed for VGG-16 model input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9f47d599-2d0f-4c77-9b0e-0b286f95a8ec",
        "_cell_guid": "2ab47e8c-92da-4105-b4d9-b3815f585c9a",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "W7rJkR2qd7e5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_imgs(set_name, img_size):\n",
        "    \"\"\"\n",
        "    Resize and apply VGG-15 preprocessing\n",
        "    \"\"\"\n",
        "    set_new = []\n",
        "    for img in set_name:\n",
        "        img = cv2.resize(\n",
        "            img,\n",
        "            dsize=img_size,\n",
        "            interpolation=cv2.INTER_CUBIC\n",
        "        )\n",
        "        set_new.append(preprocess_input(img))\n",
        "    return np.array(set_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9d2ecee5-b49f-4a20-8c2e-dcf9cf05fb9d",
        "_cell_guid": "6587c145-0170-4e66-b6c9-1c430aef9905",
        "trusted": true,
        "id": "cxbgq429d7fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\n",
        "X_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\n",
        "X_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "63939766-a698-4191-ac8a-32c3a0fb17c6",
        "_cell_guid": "8fb03076-0700-4a11-a6a2-0fef64200ab1",
        "trusted": true,
        "id": "iZsv0KZbd7fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " plot_samples(X_train_prep, y_train, labels, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "86d4aff3-f91f-4bc4-b1a9-80bc5a130982",
        "_cell_guid": "88347199-e582-473b-8211-d6bf8523bb09",
        "trusted": true,
        "id": "fatDkzq_d7fS",
        "colab_type": "text"
      },
      "source": [
        "# <a id='cnn'>4. CNN Model</a>\n",
        "\n",
        "I was using [Transfer Learning](https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e) with VGG-16 architecture , xception,InceptionV3 and weights as a base model.\n",
        "\n",
        "## <a id='aug'>4.1. Data Augmentation</a>\n",
        "\n",
        "Since I had small data set I used the technique called [Data Augmentation](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) which helps to \"increase\" the size of training set.\n",
        "\n",
        "### <a id='demo'>4.1.1. Demo</a>\n",
        "\n",
        "That's the example from one image how does augmentation look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1f54c4ed-18cd-4708-98ab-0705c3045b17",
        "_cell_guid": "dfd7cb17-08c8-4dfb-abb7-96860bb51e78",
        "trusted": true,
        "id": "kTrqtNzKd7fU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the paramters we want to change randomly\n",
        "demo_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    rescale=1./255,\n",
        "    shear_range=0.05,\n",
        "    brightness_range=[0.1, 1.5],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "22fc7ef2-a120-41a2-90fe-785e8424ff55",
        "_cell_guid": "c95e27b7-a056-4da6-9851-7c377cb48655",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "ub3MlJOrd7fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('preview')\n",
        "x = X_train_crop[0]  \n",
        "x = x.reshape((1,) + x.shape) \n",
        "\n",
        "i = 0\n",
        "for batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n",
        "    i += 1\n",
        "    if i > 20:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1b94460c-e707-4363-9a75-d112d0a21b24",
        "_cell_guid": "05583e86-3ffa-471e-a90c-5e01dbc14aa5",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "xgWIiYzkd7fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(X_train_crop[0])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title('Original Image')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "i = 1\n",
        "for img in os.listdir('preview/'):\n",
        "    img = cv2.cv2.imread('preview/' + img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(3,7,i)\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    i += 1\n",
        "    if i > 3*7:\n",
        "        break\n",
        "plt.suptitle('Augemented Images')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d4da329a-e032-4841-b687-cb540d595f00",
        "_cell_guid": "0f98d9c2-3cfa-4c3f-99a4-3815f1ed2f6c",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "InVXk182d7fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf preview/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b11eaeba-c072-46e6-a0b7-40b43be6a995",
        "_cell_guid": "adf56918-a29b-491f-91a4-04aacf133b65",
        "trusted": true,
        "id": "XaKkqZBmd7fw",
        "colab_type": "text"
      },
      "source": [
        "### <a id='apply'>4.1.2. Apply</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "96ed5ea3-5d92-4094-91f5-3dc37efffc8e",
        "_cell_guid": "a5c223fb-1765-4c70-b483-ab3da2bc62d3",
        "trusted": true,
        "id": "WQrcpMccd7f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DIR = 'TRAIN_CROP/'\n",
        "VAL_DIR = 'VAL_CROP/'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    color_mode='rgb',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    color_mode='rgb',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=16,\n",
        "    class_mode='binary',\n",
        "    seed=RANDOM_SEED\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1a12593b-4063-4232-80b8-43c586f8ff57",
        "_cell_guid": "04c5309c-39a4-4a43-af25-6eedf583005e",
        "trusted": true,
        "id": "AQRcm_DPd7f8",
        "colab_type": "text"
      },
      "source": [
        "## <a id='build'>4.2. Transfer Learning Tutorial</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "1965308d-bd96-496f-8640-39c8c56975f2",
        "_cell_guid": "2162ddcc-1824-45d7-8b94-05d790b62e54",
        "trusted": true,
        "id": "TbTOXzOtd7f-",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning Tutorial \n",
        "\n",
        "![Transfer Learning](https://cdn.elearningindustry.com/wp-content/uploads/2016/09/5-tips-improve-knowledge-transfer-elearning-e1475138920743.jpeg)\n",
        "\n",
        "> In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n",
        "\n",
        "We always hear that we **do not have to reinvent the wheel**. Well, this is always true. Why do not we work and climb over the giant shoulders? Why do not we build something even if we change it a bit?\n",
        "Well, that's not a theft. In fact, everything on the Internet without a license is open source. You can deal with a simple modification that you can get on your next research paper, but the purpose is to understand what has been completed and not just use it.\n",
        "\n",
        "\n",
        "These two major Transfer learning scenarios look as follows:\n",
        "\n",
        "**Finetuning the convnet**: Instead of random initializaion, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.\n",
        "\n",
        "**ConvNet as fixed feature extractor**: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "I was wondering a few days ago if I had 1000 pictures of a dog in a folder , but I do not know the number of types of dogs inside. just 1000 pictures only, no more and no less with out topic or file name or folder name or just a small label . how can i apply classifer method on it ?\n",
        "This was a big problem so I thought of some solutions that might be logical for most of us, namely, Transefer learning .\n",
        "What if CNN was used and at the last activation Function was removed ?\n",
        "The output must be Tensors *\n",
        "Yes, a large array of matrices carry the standard Features of each dog and that is what I want.\n",
        "Now I can use the compilation method and the elbow method to see how many dogs are in the picture\n",
        "Then we apply one of the dimensions reduction algorithms Like (PCA) and use the k-nn algorithm, and then we will have half of the non-supervisory Deep learning algorithm called \n",
        "K-CNN.\n",
        "I do not know What do you think ?\n",
        "\n",
        "\n",
        "\n",
        "Well, it's a great idea but unfortunately there are people before me who wrote the paper but it does not matter I'll invent something else and I will write a research paper someday [Class Agnostic Image Common Object Detection](https://ieeexplore.ieee.org/document/8606132)\n",
        "\n",
        "\n",
        "![Transefer Learning](https://cdn-images-1.medium.com/max/1600/1*9GTEzcO8KxxrfutmtsPs3Q.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8d051b3c-d181-4ab4-be1c-ed9e16ccaad4",
        "_cell_guid": "c6de3847-3133-465c-8ffd-70b6e2ac2b90",
        "trusted": true,
        "id": "5Tq9ZYYwd7gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# load base model\n",
        "ResNet50_weight_path = '../input/keras-pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "resnet50_x = ResNet50(\n",
        "    weights=ResNet50_weight_path,\n",
        "    include_top=False, \n",
        "    input_shape=IMG_SIZE + (3,)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ca9f548f-e4e3-4fa3-9ff2-f675832e2eda",
        "_cell_guid": "5f2877ae-58d8-4c08-8b00-73bde81edef1",
        "trusted": true,
        "id": "pq_vYUK9d7gL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load base model\n",
        "InceptionV3_weight_path = '../input/keras-pretrained-models/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "inceptionV3 = InceptionV3(\n",
        "     weights=InceptionV3_weight_path,\n",
        "    include_top=False, \n",
        "    input_shape=IMG_SIZE + (3,)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "efaa8064-d404-46b8-b695-1c0fa62e9032",
        "_cell_guid": "297e5b70-13d2-4363-a962-d30fd387d933",
        "trusted": true,
        "id": "iPJF8XuId7gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load base model\n",
        "vgg16_weight_path = '../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "vgg = VGG16(\n",
        "    weights=vgg16_weight_path,\n",
        "    include_top=False, \n",
        "    input_shape=IMG_SIZE + (3,)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f5cb32b7-e875-4b71-990f-5bec035525c9",
        "_cell_guid": "c596b3f4-9fe6-4ac6-9006-7cdc95c68908",
        "trusted": true,
        "id": "EKceiJRZd7gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import math\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import umap\n",
        "from PIL import Image\n",
        "from scipy import misc\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "from scipy import misc\n",
        "from random import shuffle\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "75ae3b26-5e0e-4095-a290-8aedf7e74725",
        "_cell_guid": "a51800d9-0d6c-479e-ab24-c48345fc866f",
        "trusted": true,
        "id": "nB_Sh3I5d7gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dropout, Lambda\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "33c38551-8874-4eae-9f71-1e62ab990875",
        "_cell_guid": "6af6b439-e615-4475-b0e8-79c022bab602",
        "trusted": true,
        "id": "_k_W0JEId7go",
        "colab_type": "text"
      },
      "source": [
        "I also love the scholars to learn from them and their experiences, and to give me stories about them and their experiences in life.\n",
        "I like their way of telling stories and giving me information, they sing for hours of hard and long work\n",
        "\n",
        "This is exactly what happens in a model that has been trained on a lot of things. What we are doing now is to give out only two of the 1000 things that we know and ask. Do you know them?\n",
        "Well, what would happen if we did not use weights for the model?\n",
        "In that case it will be a normal model all you do is put it in your form and add the final layer **Flatten**  to initialize the model for the classification process\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0fd9f90e-5909-488e-ad1d-84038c0b6f88",
        "_cell_guid": "3004d852-9399-491e-8080-a6f5395e37a2",
        "trusted": true,
        "id": "3R2Fs__Yd7gp",
        "colab_type": "text"
      },
      "source": [
        "# VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4770b35f-8f19-4386-b902-ef57276c7463",
        "_cell_guid": "fd1f6647-bcc6-41ad-b520-5ea2f02f0643",
        "trusted": true,
        "id": "DaC_Cp3Ld7gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot feature map of first conv layer for given image\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot \n",
        "from numpy import expand_dims\n",
        "\n",
        "\n",
        "f = plt.figure(figsize=(16,16))\n",
        "# load the modelf = plt.figure(figsize=(10,3))\n",
        "model = VGG16()\n",
        "# redefine model to output right after the first hidden layer\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
        "model.summary()\n",
        "# load the image with the required shape\n",
        "# convert the image to an array\n",
        "img = img_to_array(X_val_prep[43])\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# prepare the image (e.g. scale pixel values for the vgg)\n",
        "img = preprocess_input(img)\n",
        "# get feature map for first hidden layer\n",
        "feature_maps = model.predict(img)\n",
        "# plot all 64 maps in an 8x8 squares\n",
        "square = 8\n",
        "ix = 1\n",
        "for _ in range(square):\n",
        "\tfor _ in range(square):\n",
        "\t\t# specify subplot and turn of axis\n",
        "\t\tax = pyplot.subplot(square, square, ix)\n",
        "\t\tax.set_xticks([])\n",
        "\t\tax.set_yticks([])\n",
        "\t\t# plot filter channel in grayscale\n",
        "\t\tpyplot.imshow(feature_maps[0, :, :, ix-1], cmap='viridis')\n",
        "\t\tix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f1b0b7f4-4828-416e-bf29-df25d068773a",
        "_cell_guid": "b39680d5-61d3-456a-8a29-8c3270c989ce",
        "trusted": true,
        "id": "W3z6MqTdd7g3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "vgg16 = Sequential()\n",
        "vgg16.add(vgg)\n",
        "vgg16.add(layers.Dropout(0.3))\n",
        "vgg16.add(layers.Flatten())\n",
        "vgg16.add(layers.Dropout(0.5))\n",
        "vgg16.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n",
        "\n",
        "vgg16.layers[0].trainable = False\n",
        "\n",
        "vgg16.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=RMSprop(lr=1e-4),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "vgg16.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), metrics=[\"accuracy\"])\n",
        "\n",
        "vgg16.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "bfa972fb-c5e4-4c97-9ebb-9880925ac351",
        "_cell_guid": "90c09d38-bb46-4faa-87e5-60244fb53114",
        "trusted": true,
        "id": "nZF6LK_cd7g9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize feature maps output from each block in the vgg model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import expand_dims\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# load the model\n",
        "model = VGG16()\n",
        "# redefine model to output right after the first hidden layer\n",
        "ixs = [2, 5, 9, 13, 17]\n",
        "outputs = [model.layers[i].output for i in ixs]\n",
        "model = Model(inputs=model.inputs, outputs=outputs)\n",
        "# load the image with the required shape\n",
        "# convert the image to an array\n",
        "img = img_to_array(X_val_prep[43])\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# prepare the image (e.g. scale pixel values for the vgg)\n",
        "img = preprocess_input(img)\n",
        "# get feature map for first hidden layer\n",
        "feature_maps = model.predict(img)\n",
        "# plot the output from each block\n",
        "square = 8\n",
        "for fmap in feature_maps:\n",
        "\t# plot all 64 maps in an 8x8 squares\n",
        "\tix = 1\n",
        "\tfor _ in range(square):\n",
        "\t\tplt.figure(figsize=(64,64))\n",
        "\t\tfor _ in range(square):\n",
        "           \n",
        "\n",
        "\t\t\t# specify subplot and turn of axis\n",
        "\t\t\tax = pyplot.subplot(square, square, ix)\n",
        "\t\t\tax.set_xticks([])\n",
        "\t\t\tax.set_yticks([])\n",
        "\t\t\t\n",
        "\t\t\t# plot filter channel in grayscale\n",
        "\t\t\tplt.imshow(fmap[0, :, :, ix-1], cmap='viridis')\n",
        "\t\t\tix += 1\n",
        "\t# show the figure\n",
        "\n",
        "        \n",
        "\tplt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7d6d23e2-26f8-49b8-88b0-099d4a9402b2",
        "_cell_guid": "17904adb-9cd2-45dd-a682-a92d5cd39c3c",
        "trusted": true,
        "id": "KN8fTW_id7hC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "vgg16_history = vgg16.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=50,\n",
        "    epochs=120,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=30,\n",
        ")\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5884b532-8f00-49e9-9359-4e14b7429d43",
        "_cell_guid": "fae48685-f5a3-4f19-ad90-733448d80774",
        "trusted": true,
        "id": "7DJmnu0rd7hI",
        "colab_type": "text"
      },
      "source": [
        "## Calculate Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cfe67272-3075-483f-b30f-8167e9907d91",
        "_cell_guid": "983da42f-6df5-42ba-a84e-7577366de95e",
        "trusted": true,
        "id": "U96Y8C5kd7hK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validate on val set\n",
        "predictions = vgg16.predict(X_test_prep)\n",
        "predictions = [1 if x>0.5 else 0 for x in predictions]\n",
        "\n",
        "_, train_acc = vgg16.evaluate(X_val_prep, y_val, verbose=0)\n",
        "_, test_acc = vgg16.evaluate(X_test_prep, y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "21ef61f2-f333-4aa7-9eb0-0fa0b5864c1d",
        "_cell_guid": "f1d79f34-30fd-41da-89ff-d9d79875a6ff",
        "trusted": true,
        "id": "3QIYCF2Cd7hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot.figure(figsize=(12,12))\n",
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Vgg16 Loss')\n",
        "pyplot.plot(vgg16_history.history['loss'], label='train')\n",
        "pyplot.plot(vgg16_history.history['val_loss'], label='Validation')\n",
        "pyplot.legend()\n",
        "# plot accuracy during training\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Vgg16 Accuracy')\n",
        "pyplot.plot(vgg16_history.history['acc'], label='train')\n",
        "pyplot.plot(vgg16_history.history['val_acc'], label='Validation')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "759edb69-6d1e-4909-bc2a-c14698a3125d",
        "_cell_guid": "15dba75c-e437-4456-99f8-a5b2f34acbce",
        "trusted": true,
        "id": "JUNUo1pcd7hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3136b2e3-e724-4f60-bd29-507d18ad380b",
        "_cell_guid": "77337848-1afa-45d6-9dac-dbcbb3e69f6d",
        "trusted": true,
        "id": "WeyduwoTd7hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, predictions)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test, predictions)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "05de6478-d7a6-4899-92ce-89690b1519b9",
        "_cell_guid": "a7463d36-3444-42b5-b875-f497b1014291",
        "trusted": true,
        "id": "bP5yH_wPd7h0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kappa = cohen_kappa_score(y_test, predictions)\n",
        "print('Cohens kappa: %f' % kappa)\n",
        "# ROC AUC\n",
        "auc = roc_auc_score(y_test, predictions)\n",
        "print('ROC AUC: %f' % auc)\n",
        "# confusion matrix\n",
        "matrix = confusion_matrix(y_test, predictions)\n",
        "print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b088575d-966d-412c-92c2-7576a1ae63d6",
        "_cell_guid": "81b69ba6-1217-43c0-b2aa-8509b11bd9f7",
        "trusted": true,
        "id": "kcZ6pxg8d7iD",
        "colab_type": "text"
      },
      "source": [
        "# InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "61b52144-c0e3-4b7c-9a18-088041cec6f8",
        "_cell_guid": "e1dde7ee-1c3f-428c-8d98-b85d0271bc6e",
        "trusted": true,
        "id": "7UlGfXUYd7iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "inception_v3 = Sequential()\n",
        "inception_v3.add(inceptionV3)\n",
        "inception_v3.add(layers.Dropout(0.3))\n",
        "inception_v3.add(layers.Flatten())\n",
        "inception_v3.add(layers.Dropout(0.5))\n",
        "inception_v3.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n",
        "\n",
        "inception_v3.layers[0].trainable = False\n",
        "\n",
        "inception_v3.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=RMSprop(lr=1e-4),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "inception_v3.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "edac9ac9-3ec6-4461-bb6b-72df8b966ddf",
        "_cell_guid": "c79e1a42-4a73-4a93-b30e-028cd0a08d9d",
        "trusted": true,
        "id": "wfMRp-u8d7iM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "inception_v3_history = inception_v3.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=50,\n",
        "    epochs=120,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=30,\n",
        ")\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "dbfe3853-b4b1-4d82-a355-ec757df003f2",
        "_cell_guid": "5fb66d99-2524-4e1b-8ba5-4e08e49cf952",
        "trusted": true,
        "id": "9YcqlVbtd7iV",
        "colab_type": "text"
      },
      "source": [
        "# RESNET50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a3905e1f-d8d8-4823-b32d-65f0e49a67b5",
        "_cell_guid": "c8523215-b5d0-4ac8-91fc-ed051b051c62",
        "trusted": true,
        "id": "LJIqVbKqd7iX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "resnet50 = Sequential()\n",
        "resnet50.add(resnet50_x)\n",
        "resnet50.add(layers.Dropout(0.3))\n",
        "resnet50.add(layers.Flatten())\n",
        "resnet50.add(layers.Dropout(0.5))\n",
        "resnet50.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n",
        "\n",
        "resnet50.layers[0].trainable = False\n",
        "\n",
        "resnet50.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=RMSprop(lr=1e-4),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "resnet50.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), metrics=[\"accuracy\"])\n",
        "\n",
        "resnet50.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4eed8cb9-1ab8-4a67-9c9d-bc6588838fa5",
        "_cell_guid": "3e2c2fe9-b7ff-40cf-8068-91a773919b68",
        "trusted": true,
        "id": "0alqo5nSd7if",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "resnet50_history = resnet50.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=50,\n",
        "    epochs=120,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=30,\n",
        ")\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4de15e72-bad4-4ffc-8c9b-8eddd9648a8b",
        "_cell_guid": "a6aff669-20b2-4d61-a2a7-9dc2c92c20f0",
        "trusted": true,
        "id": "aGiqR0I5d7ir",
        "colab_type": "text"
      },
      "source": [
        "## <a id='perf'>4.3. Model Performance</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0406cea5-6405-4638-a541-fca04c0a83be",
        "_cell_guid": "bfb64fee-5637-47d3-a2d5-7a25dfcb029e",
        "trusted": true,
        "id": "Mzm8T2sXd7it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_1= vgg16_history\n",
        "history_2=inception_v3_history\n",
        "history_3=resnet50_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5b0580e6-10f8-4583-b3dd-8cb4765c2612",
        "_cell_guid": "89044887-addb-455e-a6c9-31cee80709ad",
        "trusted": true,
        "id": "UwRY5j23d7i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ModelGraphTrainngSummary(history,N,model_name):\n",
        "    # set the matplotlib backend so figures can be saved in the background\n",
        "    # plot the training loss and accuracy\n",
        "    import sys\n",
        "    import matplotlib\n",
        "    print(\"Generating plots...\")\n",
        "    sys.stdout.flush()\n",
        "    matplotlib.use(\"Agg\")\n",
        "    matplotlib.pyplot.style.use(\"ggplot\")\n",
        "    matplotlib.pyplot.figure()\n",
        "    matplotlib.pyplot.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
        "    matplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
        "    #matplotlib.pyplot.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\n",
        "    #matplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\n",
        "    matplotlib.pyplot.title(\"Training Loss and Accuracy on Brain Tumor Classification\")\n",
        "    matplotlib.pyplot.xlabel(\"Epoch #\")\n",
        "    matplotlib.pyplot.ylabel(\"Loss/Accuracy of \"+model_name)\n",
        "    matplotlib.pyplot.legend(loc=\"lower left\")\n",
        "    matplotlib.pyplot.savefig(\"plot.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "504576a9-74a9-4732-a139-2afe9144f231",
        "_cell_guid": "078c46c3-f6dc-4bad-91c1-73a438e4dca4",
        "trusted": true,
        "id": "9TN3KGBjd7i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ModelGraphTrainngSummaryAcc(history,N,model_name):\n",
        "    # set the matplotlib backend so figures can be saved in the background\n",
        "    # plot the training loss and accuracy\n",
        "    import sys\n",
        "    import matplotlib\n",
        "    print(\"Generating plots...\")\n",
        "    sys.stdout.flush()\n",
        "    matplotlib.use(\"Agg\")\n",
        "    matplotlib.pyplot.style.use(\"ggplot\")\n",
        "    matplotlib.pyplot.figure()\n",
        "    #matplotlib.pyplot.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
        "    #matplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
        "    matplotlib.pyplot.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\n",
        "    matplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\n",
        "    matplotlib.pyplot.title(\"Training Loss and Accuracy on Brain Tumor Classification\")\n",
        "    matplotlib.pyplot.xlabel(\"Epoch #\")\n",
        "    matplotlib.pyplot.ylabel(\"Accuracy of \"+ model_name)\n",
        "    matplotlib.pyplot.legend(loc=\"lower left\")\n",
        "    matplotlib.pyplot.savefig(\"plot.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9bf92148-7bcc-4adc-aa68-6c2f26f67bb2",
        "_cell_guid": "76e03eea-1177-4d9e-a73a-aaabd95f5f84",
        "trusted": true,
        "id": "zgg3LGXWd7i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x_model in [{'name':'VGG-16','history':history_1,'model':vgg16},\n",
        "                {'name':'Inception_v3','history':history_2,'model':inception_v3},\n",
        "                {'name':'Resnet','history':history_3,'model':resnet50}]:\n",
        "    ModelGraphTrainngSummary(x_model['history'],120,x_model['name'])\n",
        "    ModelGraphTrainngSummaryAcc(x_model['history'],120,x_model['name'])\n",
        "    \n",
        "    # validate on val set\n",
        "    predictions = x_model['model'].predict(X_val_prep)\n",
        "    predictions = [1 if x>0.5 else 0 for x in predictions]\n",
        "\n",
        "    accuracy = accuracy_score(y_val, predictions)\n",
        "    print('Val Accuracy = %.2f' % accuracy)\n",
        "\n",
        "    confusion_mtx = confusion_matrix(y_val, predictions) \n",
        "    cm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c3b3060b-59ca-4411-a1b5-689ab35b624e",
        "_cell_guid": "95323fc5-e8db-4839-80ac-6168a202a6ba",
        "trusted": true,
        "id": "3IU6AHHEd7jB",
        "colab_type": "text"
      },
      "source": [
        "# <a id='concl'>5. Conclusions</a>\n",
        "\n",
        "This project was a combination of CNN model classification problem (to predict wheter the subject has brain tumor or not) & Computer Vision problem (to automate the process of brain cropping from MRI scans). The final accuracy is much higher than 50% baseline (random guess). However, it could be increased by larger number of train images or through model hyperparameters tuning.\n",
        "\n",
        "Well we've done very well just about a cycle brother just that it's a record I think I'm happy with that result and I'm also happy to share some science with you.\n",
        "Now it is your turn to use lots and lots of your own models, you can publish your next research paper entitled Transefer Learning .\n",
        "This was a great lesson and thank you for following up and thank this man for his efforts at this kernel\n",
        "\n",
        " Ruslan Klymentiev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b11a6e84-a266-417c-88ee-cc93ce218436",
        "_cell_guid": "508ffc46-2a35-4e1f-926a-578f467b7a13",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "2P16t7wgd7jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean up the space\n",
        "!rm -rf TRAIN TEST VAL TRAIN_CROP TEST_CROP VAL_CROP\n",
        "# save the model\n",
        "\n",
        "\n",
        "vgg16.save('2019-8-6_VGG_model.h5')\n",
        "inception_v3.save('2019-8-6_inception_v3.h5')\n",
        "resnet50.save('2019-8-6_resnet50.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}